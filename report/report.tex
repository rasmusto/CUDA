%LaTeX Header
\documentclass[10pt]{article}
\usepackage[letterpaper]{geometry}
\geometry{top=1.0in, bottom=1.0in, left=1.0in, right=1.0in}
\usepackage{times}
%\usepackage{amssymb,amsmath}
\usepackage{amssymb}
\usepackage{mathtools}%added this
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{listings}
%\usepackage{fontspec}

\usepackage[hang,small,bf]{caption}

\usepackage{multicol}
\usepackage{minted}

%\setsansfont{Calibri}
%\setmonofont{Consolas}

\usemintedstyle{solarized}

\usepackage{booktabs}

\renewcommand{\arraystretch}{1.2}

\renewcommand\theFancyVerbLine{\small\arabic{FancyVerbLine}}

\lstset{frame=single, basicstyle=\footnotesize, breaklines=true, numbers=left}

% Title Page
\title{CUDA Implementation of Jacobi Relaxation}
\author{Neeraja Budamagunta, Torben Rasmussen, Sweta Sharma, John Wehland, Matthew Wolfe}
\date{\today}

\makeatletter
\newenvironment{tablehere}
{\def\@capttype{table}}
{}

\newenvironment{figurehere}
{\def\@captype{figure}}
{}
\makeatother

\begin{document}
\renewcommand{\theFancyVerbLine}{
\sffamily\textcolor[rgb]{0.5,0.5,0.5}{\scriptsize\arabic{FancyVerbLine}}}

\maketitle

%\tableofcontents
%\clearpage

\begin{multicols}{2}
  \begin{abstract}
    CUDA is a extension on C++ that allows for massively parallel programming (MPP).
    This paper describes optimizations made to a Jacobi Relaxation program written by Dr. Wolfe.
    His implementation was debugged and cleaned to allow for its correct execution
    It was then converted to use only one GPU kernel, and further modified to allow the user specify what size matrix and how many threads to use.
    Then, functionality was added that allowed the collection of performance data.
    Also, the Nvidia CUDA profiler was used to give performance data.
    That data was tabulated, collected and now presented to the reader.
  \end{abstract}
  \section{Introduction} %background/context, the idea, summary of results
  \label{sec:introduction}
  The graphics processing unit (GPU) is an application-specific device aimed at rapidly building images for viewing on a display.
  Over the past decade, GPUs have become more and more general purpose, and can now be called general purpose GPUs (GP-GPUs).
  Software frameworks such as CUDA and OpenCL have allowed researchers to tap into this parallelism.
  These frameworks allow the creation of parallel mathematical applications that can be deployed on low-cost, readily available hardware.
  CUDA, or Compute Unified Device Architecture, is Nvidia's parallel computing architecture.
  This architecture gives a programmer access to the underlying hardware through a few layers of abstraction, allowing for relatively high-level programming.
  A GPU can offer a very high computational rate if the algorithm is well-suited for the device.
  One such application is the Jacobi relaxation.
  This method works on a matrix input, which means it is parallelizable.
  Hence, it is well suited to work with on GP-GPU’s.

  Our implementation of this method is largely based off of the work of Dr. Michael Wolfe.  
  His version of the Jacobi Relaxation for the CUDA architecture acted as a framework and reference for our design.
  Our first step was to debug, run and benchmark Dr. Wolfe's code.
  Dr. Wolfe's code implemented the Jacobi Relaxation and reduction in separate kernels, so one of our optimizations involved combining these into a single kernel.
  In addition, we modified his two-kernel and our one-kernel implementations to take input matrix sizes that were not multiples of \(16 + 2\), and also to change the block size.

  The rest of this paper is organized in the following way:
  Section \ref{sec:design} talks about our project goals, design and solution to our problem.
  Section \ref{sec:results} shows the actual implementations followed by the results achieved.
  We conclude in section \ref{sec:conclusion}.
  We had limited goals due to shortage of time and resources, so we discuss future work in \ref{sec:future_work}.

  \section{System Specifications}
  \subsection{GPU Specifications}
  The GPU we used to run and benchmark our implementations was an Nvidia Tesla C1060.
  See table~\ref{tb:tesla} for more detail.

  \begin{table*}[!ht]\centering
    \begin{tabular}{@{}l l@{}}\toprule
      \bf{Spec}                       &   \bf{Value}          \\
      \hline
      Compute Capability              &   1.3                 \\
      Dedicated global memory         &   4294770688 (4GiB)   \\
      Total Constant Memory           &   65536 (64kiB)       \\
      Scalar Processors per GPU       &   240                 \\
      Streaming Multiprocessor Count  &   30                  \\
      Shared memory per MP            &   16384 (16kiB)       \\
      Registers per MP                &   16384 (16kiB)       \\
      Threads per warp                &   32                  \\
      Max threads per block           &   512                 \\
      Max thread dimensions           &   (512 512 64)        \\
      Max Grid dimensions             &   (65535 65535 1)     \\
      \hline
    \end{tabular}
    \caption{Nvidia Tesla C1060 Specifications}
    \label{tb:tesla}
  \end{table*}

  \subsection{CPU Specifications}
  The computer used for this project ``Meakin'', contains an Intel Xeon Processor. More detailed specifications are found in table \ref{tb:cpu} \cite{bib:xeon}.
  \begin{table*}[!ht]\centering
    \begin{tabular}{@{}l l@{}}\toprule
      \bf{Spec}                     &   \bf{Value}                      \\
      \hline
      Processor                     &   Intel Xeon E5504 ``Gainestown'' \\
      Number of Cores (threads)     &   4 (8)                           \\
      CPU Clock Rate                &   2.0 GHz                         \\
      L3 Cache                      &   4 MB                            \\
      Memory Interface              &   3x DDR3-800                     \\
      System Memory                 &   12290000 kB $\approx$ 12 GB     \\
      \hline
    \end{tabular}
    \caption{Meakin CPU specifications}
    \label{tb:cpu}
  \end{table*}

  \section{Design}
  \label{sec:design}
  \subsection{The Jacobi Relaxation}
  The Jacobi Relaxation is a commonly used iterative method for solving systems of equations.
  It uses the main, upper, and lower diagonals of the matrix.
  It is able to parallelize the computations on individual elements of the matrix, which makes it very worthwhile to implement on CUDA architecture.

  The general form for a system of linear equations is (in matrix form):
  \[A*u=f\]
  Our particular implementation of the relaxation uses a weight \(w\) to help it converge more quickly:
  \[x^{(i)} = x^{(i-1)} * (1-w)+w*(D^{-1}(f-(L+U))*x^{(i-1)}\]
  The remaining parameters will now be discussed in more detail.
  \(A\) and \(f\) are the known equation coefficients and constant solutions.
  \(u\) is the unknown solution matrix.
  \(x^{(0)}\) is an initial guess for the matrix u.
  \(x^{(i)}\) is the \(i^{th}\) iteration of the Jacobi relaxation.
  The algorithm is said to be done when the solution converges, or when the solution is within a predetermined error boundary.
  Each previous value of \(x^{(i)}\) is used in the next iteration of the algorithm.
  \(D\) is composed of a matrix of the same size as \(A\), with zeros in all places besides the main diagonal:
  \[
  D =
  \begin{bmatrix*}
    A_{1,1}     & 0         & 0 \\
    0           & \ddots    & 0 \\
    0           & 0         & A_{i,i}
  \end{bmatrix*}
  \]
  \(D^{-1}\) is simply the matrix where each element of \(D\) is inverted:
  \[
  D^{-1} =
  \begin{bmatrix*}
    \frac{1}{A_{1,1}}   & 0         & 0 \\
    0                   & \ddots    & 0 \\
    0                   & 0         & \frac{1}{A_{i,i}}
  \end{bmatrix*}
  \]
  The lower and upper diagonal are the diagonals that run parallel to the main diagonal:
  \[
  A=
  \begin{bmatrix*}
    D & U & x \\
    L & D & U \\
    x & L & D
  \end{bmatrix*}
  \]
  \[
  L=
  \begin{bmatrix*}
    0 & 0               & 0 \\
    A_{0,1} & 0         & 0 \\
    0       & A_{1,2}   & 0
  \end{bmatrix*}
  ,
  U=
  \begin{bmatrix*}
    0 & A_{1,0} & 0       \\
    0 & 0       & A_{2,1} \\
    0 & 0       & 0
  \end{bmatrix*}
  \]

  In CUDA, the iterations must still run sequentially (because it is a recursive algorithm).
  Each thread works on a single element of the matrix for a given iteration, in parallel.
  When all of the threads for a given iteration are complete, the next iteration can be run.

  \subsection{Project Goals}
  The main aim of our project was to implement an optimized version of Jacobi relaxation in CUDA.
  We had Dr. Wolfe’s code \cite{bib:wolfe} as a starting point.
  After analyzing Dr. Wolfe’s code and having a good understanding of the weighted Jacobi algorithm we moved to our next step, optimization.
  Dr. Wolfe himself has used many optimizations in his code and the code was tuned for array sizes multiples of sixteen plus two.
  %The reason that it had the best performance was that the architecture block sizes of 16x16. 

  In our analysis we found two possible modifications.
  Firstly, Dr. Wolfe’s code had a bug and array sizes bigger than 258 were not converging.
  A relatively long amount of time was spent fixing the bug.
  We used last two versions of his code to create two sub-versions of code called original\_jacobi5.cu and original\_jacobi6.cu 
  These were bug-free versions with the added ability for runtime performance data measurements.

  Secondly, we found that the code is highly tuned for a particular array size and only works with a block size of 16x16.
  We wanted to change that to take user specified block size, allowing for a more generic, but still optimized program.
  Hence, we created another code sub-version which we called as original\_jacobi6Mod.cu.
  All of these sub-versions fall into the main version which we call as version 1, which is a two kernel version.

  We also implemented another version with a single kernel.
  This again had the same sub versions, the only difference being that these use only one kernel call instead of two.
  The sub versions are called 1k\_jacobi5.cu, 1k\_jacobi6.cu and 1k\_jacobi6Mod.cu, similar to the ones above.
  We discuss our implementation of the one kernel version and the modified code with user defined block sizes in the next section.
  Also we discuss how the performance of all of these six sub-versions compare.

  \section{Results}
  \label{sec:results}
  \subsection{Implementation}
  In the design section above, we discussed the algorithm and design of our solution.
  In this section we show the actual implementation and the results obtained using these optimizations.
  Two attempts to optimize Dr. Wolfe's code \cite{bib:wolfe} were performed: a single kernel version and a version where the throughput was changed according to the array size by using user entered block size values.

  \subsubsection{One Kernel Implementation}
  Our first attempt at optimization was to implement Jacobi relaxation in CUDA with one kernel.
  Dr. Wolfe had implemented Jacobi relaxation in CUDA using two kernel calls per iteration \cite{bib:wolfe}.
  Instead, as an optimization, we use a single kernel call per iteration reducing the overhead to initiate an extra kernel each time.
  The reason for using the second kernel was to reduce the change values across the blocks to one single value.
  In the first kernel, ``jacobikernel'', the change values were reduced from one change value per thread to a change value per block.
  In the second kernel, which had fewer threads and one single block, those values are reduced to one single change value.
  In our implementation we used the existing threads to do the additional work of further reducing the per block change values to a single value.

  With this optimization we were able to reduce the calculation time to a max of 10\% for smaller array sizes.
  However as the array sizes grew, an increasing number threads were idle as they were waiting for the other threads to perform the reduction.
   Performance gains through merging the kernels were trivial compared to the time lost waiting for these threads to complete.
  However, this is a possible optimization that did give us positive results for smaller array sizes and still has some potential.
  
  \subsubsection{Implementation with varying block sizes}
  In the version of Dr Wolfe's code that used the shared memory and array allocation in local memory (original\_jacobi6.cu), the versions that we implemented (1k\_jacobi6.cu), specific shared memory is allocated in each block to accommodate the ``change'' value obtained in the relaxation.
  It is impossible to dynamically allocate memory on the device, so we had to allocate enough memory to accommodate the largest possible number of threads.
  Since the threads were allocated in a 2 dimensional square and the maximum number of threads was 512, we allocated enough memory for a 22 by 22 square block.

  Here are some results captured for varying block sizes:

  %Original J6 Mod  Graph 
  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/b1.pdf}
    \caption{}
    \label{fig:2kmod_time}
  \end{figurehere}

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/c1.pdf}
    \caption{}
    \label{fig:1kmod_time}
  \end{figurehere}

  As we see in figures \ref{fig:2kmod_time} and \ref{fig:1kmod_time}, a block size of 10x10 took the most time to converge.
  Also, block sizes of 14x14 and 20x20 performed more slowly than 16x16.
  From the above implementation, we could see that the block size of 16x16 worked the best.
  This is mostly because, while GPUs are exceptional at performing many small calculations very quickly, they do not excel at memory access.
  This is a general rule, however.
  The goal is to maximize the fraction of streaming multiprocessor (SM) resources that are being used at any given time.
  One guideline is that as the complexity of the kernel increases, the thread count should decrease, but this is not always beneficial.

  As can be seen from the data collected, performance generally increases across all benchmarks as the thread count goes up.
  However, this increase has a limit.
  Once the threads begin to exceed 16x16 we begin to see a decrease in performance.
  This is because we use tiling to achieve maximum performance out of the program, which almost always performs best when 256 (16 x 16) threads are used.
  In devices of compute capability 1.x, global memory is accessed in chunks of half a warp, or 16 threads.
  By using a number of threads that is a multiple of 16 you can maximize the amount of memory read per memory fetch.
  Since a block requires a square matrix in order to perform the Jacobi relaxation algorithm, a 16x16 sized block will give the best performance.

  \subsection{Performance Statistics}
  Performance optimization revolves around the following strategies \cite{bib:nvidia}:
  \begin{itemize}
    \item Maximize parallel execution to achieve maximum utilization
    \item Optimize memory usage to achieve maximum memory throughput 
    \item Optimize instruction usage to achieve maximum instruction throughput 
  \end{itemize}

  The strategies that yield the best performance gain for a particular portion of an application depends on the performance limiters for that specific portion of the code.
  Optimizing the instruction usage of a kernel that is mostly limited by memory accesses will not yield any significant performance gains \cite{bib:nvidia}.

  Although we were not able to optimize our implementation to a great extent focusing on the above factors, we project that recording the statistics associated with the strategies listed above helped us understand the behavior of the kernels.

  We measured the time taken by the GPU kernel and the overall time taken by the program.
  Also, we measured factors like GFLOPS, memory/instruction throughput, and speed up, that would project the performance. 

  \subsubsection{Timing}

  We directly measured the elapsed time using two methods: the built-in gettimeofday() function and CUDA Events.

  \paragraph{gettimeofday()}
  This function obtains the current time, expressed as seconds and microseconds since the Epoch, and stores it in the timeval structure pointed to by tv.
  Here is the sample code that we used:

  \cite{bib:wolfe}
  \begin{minted}[fontsize=\footnotesize]{c}
    gettimeofday(&tt1, NULL);
    JacobiGPU(a, n, m, .2, .1, .1, .1);
    gettimeofday(&tt2, NULL);
    ms = (tt2.tv_sec - tt1.tv_sec);
    ms = ms * 1000000 + (tt2.tv_usec \
    - tt1.tv_usec);
    fms = (float)ms / 1000000.0f;
    printf( "time(gpu ) \
    = %f seconds\n", fms );
  \end{minted}

  \paragraph{CUDA Events}
  This API provides calls that create and destroy events, record events (via timestamp), and convert timestamp differences into a floating-point value in milliseconds.
  We used cudaEventRecord() and cudaEventElapsedTime() to measure the time taken by the GPU.
  CudaEventRecord() records an event and CudaEventElapsedTime() computes the elapsed time between two events (in milliseconds with a resolution of around 0.5 microseconds).
  Here is the sample code that we used before and after the kernel call: 
  
  \cite{bib:wolfe}
  \begin{minted}[fontsize=\footnotesize]{c}
    cudaEventRecord(e2);
    cudaMemcpy(&change, lchange, \
    sizeof(float), cudaMemcpyDeviceToHost);
    cudaEventElapsedTime(&msec, e1, e2);
    sumtime += msec;
  \end{minted}

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/a1.pdf}
    \caption{}
    \label{fig:all_time}
  \end{figurehere}

  As you see in figure \ref{fig:all_time}, the original\_jacobi5.cu and 1k\_jacobi5.cu versions took the longest time and the original\_jacobi6.cu 1k\_jacobi6.cu versions, which used the shared memory and the local thread memory for allocating the matrix and calculating the change, took the least amount of time.

  \subsubsection{Shared Memory Allocation}%john
  In the most advanced version of Dr. Wolfe's code and in the versions that we ended up implementing, specific shared memory is allocated in each block to accommodate the calculations of each thread.
  Since it is impossible to dynamically allocate memory on the device, we had to allocate enough memory to accommodate the largest possible number of threads.
  Since the threads were allocated in a 2 dimensional square and the maximum number of threads was 512, we had to allocate enough memory for a 22 by 22 square block.
  This memory was allocated into the shared memory of each block regardless of how much of that memory was actually used.
  It was only necessary to make this change for program versions that allowed for different numbers of threads per block, of course.

  As you can see from the occupancy graph, this had the effect of increasing the occupancy of the programs, allowing for multiple threads.
  However, these programs achieved consistently lower GFLOPS as well as higher GPU calculation times.
  This illustrates the fact that an increase in occupancy does not automatically lead to an increase in performance.
  In our case this is confirmation that the bottleneck in our program is not in memory.
  At any given time there is probably a thread that has already fetched its required memory and is ready to run on the SM.

  \subsubsection{GFLOPS}
  This is one of the measures of a computer's performance is nothing but floating point operations per second.
  The FLOPS can be calculated in two different ways:
  \begin{enumerate}
    \item Counting the number of operations in the code manually
    \item Counting using the PTX file
  \end{enumerate}
  We ended doing it manually, and confirmed this value by looking at the PTX file.
  Figure \ref{fig:gflops1} shows the GFLOPS obtained by all versions of the code for a block size of 16x16.
  The original\_jacobi6.cu version performed the best at large matrix sizes.

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/a2.pdf}
    \caption{GFLOPS measurement for all versions of code for block size 16x16}
    \label{fig:gflops1}
  \end{figurehere}

  Figure \ref{fig:gflops2} shows the GFLOPS performance of the two kernel implementation for various block sizes: 10x10, 14x14, 16x16, and 20x20.
  As has been stated, the 16x16 version performs the best.

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/b2.pdf}
    \caption{GFLOPS measurement with different block sizes for two kernel implementation}
    \label{fig:gflops2}
  \end{figurehere}

  \ref{fig:gflops3} shows the GFLOPS performance of the one kernel implementation for various block sizes: 10x10, 14x14, 16x16, and 20x20.

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/c2.pdf}
    \caption{GFLOPS measurement with different block sizes for one kernel implementation}
    \label{fig:gflops3}
  \end{figurehere}

  \subsubsection{Throughput}
  We captured the instruction throughput and the global memory throughput given by the CUDA Profiler.

  \paragraph{Instruction Throughput}
  Instruction throughput is the ratio of achieved instruction rate versus peak single-issue instruction rate.

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/g1.pdf}
    \caption{Instruction throughput of the three implementations of the Jacobi kernel}
    \label{fig:jacobi_instr_throughput}
  \end{figurehere}

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/g2.pdf}
    \caption{Instruction throughput of the two implementations of the reduction kernel}
    \label{fig:reduction_instr_throughput}
  \end{figurehere}

  As we see from figures \ref{fig:jacobi_instr_throughput} and \ref{fig:reduction_instr_throughput}, the one kernel version gives a better instruction throughput for smaller matrix sizes and is dominated by the two kernel version as the matrix size increases.
  Also, the throughput for the reduction kernel decreases as the  matrix size increases because there are an increasing number of threads staying idle. 

  \paragraph{Global Memory Throughput}
  Global memory throughput is the sum of read and write throughput.
  Minimizing data transfers between global memory and the device by increasing use of shared memory and caches is one way to optimize throughput \cite{bib:nvidia}.

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/g3.pdf}
    \caption{Memory throughput of the three inplementations of the Jacobi Kernel}
    \label{fig:jacobi_mem_throughput}
  \end{figurehere}

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/g4.pdf}
    \caption{}
    \label{fig:reduction_mem_throughput}
  \end{figurehere}
  As we see from figures \ref{fig:jacobi_mem_throughput} and \ref{fig:reduction_mem_throughput}, the global memory throughput is high for original\_jacobi5.cu and original\_jacobi6.cu.
  Original\_jacobi5.cu uses the device's global memory, and original\_jacobi6.cu makes use of the shared memory and the local thread memory for the array initialization and relaxation 
  This leads to a significant decrease in the global memory throughput.

  \subsubsection{Occupancy}
  Occupancy is the ratio of active warps to the maximum number of warps supported on a multiprocessor of the GPU.
  Each multiprocessor on the device has a set of N registers available for use by CUDA program threads.
  These registers are a shared resource that are allocated among the thread blocks executing on a multiprocessor.
  The CUDA compiler attempts to minimize register usage to maximize the number of thread blocks that can be active in the machine simultaneously.
  Maximizing the occupancy can help to cover latency during global memory loads that are followed by a \_\_syncthreads() \cite{bib:nvidia}.

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/o1.pdf}
    \caption{Occupancy for the modified one kernel implementation for block sizes 10x10, 14x14, 16x16, and 20x20}
    \label{fig:1k_occupancy}
  \end{figurehere}

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/o2.pdf}
    \caption{Occupancy for the modified two kernel implementation for block sizes 10x10, 14x14, 16x16, and 20x20}
    \label{fig:2k_occupancy}
  \end{figurehere}

  As we see from figures \ref{fig:1k_occupancy} and \ref{fig:2k_occupancy}, the occupancy is 100\% for a block size of 16x16.
  Also, higher occupancy does not necessarily mean higher performance.
  If a kernel is not bandwidth bound, then increasing occupancy will not necessarily increase performance.
  If a kernel invocation is already running at one thread block per multiprocessor in the GPU, and it is bottlenecked by computation and not by global memory accesses, then increasing occupancy may have no effect \cite{bib:nvidia}.

  \subsubsection{Speed Up}

  \begin{figurehere}
    \centering
    \includegraphics[width=8cm]{graphs/s.pdf}
    \caption{Speed up versus sequential code}
    \label{fig:speedup}
  \end{figurehere}
  From figure \ref{fig:speedup} we see that the speed up gained over sequential by the original\_j6.cu version, which used the shared, local thread memory for relaxation and array initialization, was 130-140 for matrix sizes between 1000 and 4000.

  Speed up gained for the original\_j5.cu version over the sequential version was relatively lower, about 60-80 for similar matrix sizes.

  \section{Conclusion} %repeat idea, summarize key results
  \label{sec:conclusion}
  We have described our techniques for optimizing the code we received from the presentation given by Dr. Wolfe.
  We have also shown our methods for collecting data about the performance of the code that was received and our own extension of that code.
  It was discovered found that optimization of code that is not hardware specific is very difficult.
  In some areas our implementation does perform better than the bug-fixed version of Dr. Wolfe's code, but in others it does not.
  The results that were obtained do suggest that it would be possible to increase the speed of the calculations further, although we were not able to implement them at this time.

  \section{Future Work} %list the things you wanted to do but couldn't finish in time for this paper
  \label{sec:future_work}
  There are several areas for future work that are recognized at this time.
  Additional work needs to be done on implementing the Jacobi Method with one kernel call.
  We believe that better performance can still be achieved using the current method with further effort put into optimization.
  We would have also like to have added functionality to initialize the array on the GPU.
  When this is done in parallel it is done more quickly than the sequential version.
  Lastly, a version could be created that would customize itself to each individual GPU by getting specific information from the GPU and using that data to fine-tune the kernel.

  \section{Acknowledgements} %mention wolfe et al
  \label{sec:acknowledgements}
  We would like to thank Dr. Wolfe for the examples that he provided us.
  This work was performed on equipment allocated to us from the Portland State University CUDA lab.

\end{multicols}

\begin{flushleft}
  \begin{thebibliography}{99}
    \bibitem{bib:jacobi_wiki}``Relaxation (iterative Method).''
      Wikipedia, the Free Encyclopedia. Web. 01 Aug. 2011. $<$http://en.wikipedia.org/wiki/Relaxation\_(iterative\_method)$>$.
    \bibitem{bib:wolfe}``Jacobi Relaxation'' (unpublished)
      Dr. Michael Wolfe. The Portland Group, Inc. $<$http://www.pgroup.com/$>$
    \bibitem{bib:nvidia}``Nvidia CUDA Compute Unified Device Architecture Programming Guide''
      NVIDIA Corporation. Web. 18 Aug. 2011. $<$http://developer.download.nvidia.com/compute/cuda/2\_0/docs/NVIDIA\_CUDA\_Programming\_Guide\_2.0.pdf$>$
    \bibitem{bib:xeon_wiki}``Xeon''
      Wikipedia, the Free Encyclopedia. Web. 18 Aug. 2011. $<$http://en.wikipedia.org/wiki/Xeon$>$
    \bibitem{bib:zhang}``A Quantitative Performance Analysis Model for GPU Architectures''
      Yao Zhang and John D. Owens. Department of Electrical and Computer Engineering. University of Californa, Davis.
  \end{thebibliography}
\end{flushleft}

\clearpage

\appendix
\section{Code}
\label{sec:code}

%%include link to github repository?  http://github.com/rasmusto/CUDA

\subsection{Sequential Code}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/seq_jacobi.c}

\subsection{Dr. Wolfe's Code}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/original_jacobi5.cu}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/original_jacobi6.cu}

\subsection{One Kernel Implementation}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/1k_jacobi5.cu}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/1k_jacobi6.cu}

\subsection{Adjustable Thread Count Implementation}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/orig_J6M_v2.cu}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/1k_J6M_v2.cu}

\end{document}
