%LATEX Header
\documentclass[10pt]{article}
\usepackage[letterpaper]{geometry}
\geometry{top=1.0in, bottom=1.0in, left=1.0in, right=1.0in}
\usepackage{times}
%\usepackage{amssymb,amsmath}
\usepackage{amssymb}
\usepackage{mathtools}%added this
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{fontspec}

\usepackage{multicol}
\usepackage{minted}

\setsansfont{Calibri}
\setmonofont{Consolas}

\usemintedstyle{solarized}

\usepackage{booktabs}

\renewcommand{\arraystretch}{1.2}

\renewcommand\theFancyVerbLine{\small\arabic{FancyVerbLine}}

\lstset{frame=single, basicstyle=\footnotesize, breaklines=true, numbers=left}

% Title Page
\title{CUDA Implementation of Jacobi Relaxation}
\author{Neeraja Budamagunta, Torben Rasmussen, Sweta Sharma, John Wehland, Matthew Wolfe}
\date{\today}

\makeatletter
\newenvironment{tablehere}
    {\def\@capttype{table}}
    {}

%\newenvironment{figurehere}
%    {\def\@captype{figure}}
%    {}
%\makeatother

\begin{document}
\renewcommand{\theFancyVerbLine}{
  \sffamily\textcolor[rgb]{0.5,0.5,0.5}{\scriptsize\arabic{FancyVerbLine}}}

\maketitle
%\thispagestyle{empty}
%\clearpage

%\tableofcontents
%\setcounter{page}{1}
%\clearpage

\begin{multicols}{2}
\begin{abstract}
CUDA is a extension on C++ that allows for massively parallel programming (MPP).
In this paper we describe how we optimized a program written by Micheal Wolfe that uses a Jacobi Relaxation technique.
We debugged/cleaned his implementation to allow it to work correctly, then converted it to use only one GPU kernel, going further to allow the user specify what size matrix to use and how many threads to use.
We then add functionality that allows us to collect performance data.
Also, we use the profiler to give us performance data as well.
We tabulate that data that was collected and present this to the reader.
Finally, we propose areas of future work.
\end{abstract}

\section{Introduction} %background/context, the idea, summary of results
The graphics processing unit (GPU) is an application-specific device aimed at rapidly building images for viewing on a display.
Over the past decade, GPUs have become more and more general purpose, and can now be called general purpose GPUs (GP-GPUs).
Software frameworks such as CUDA and OpenCL have allowed researchers to tap into the parallelism that exists in these devices.
These frameworks allow the creation of parallel mathematical applications that can be deployed on low-cost, readily available hardware.
CUDA, or Compute Unified Device Architecture, is Nvidia's parallel computing architecture.
This architecture gives a programmer access to the underlying hardware through a few layers of abstraction, allowing for relatively high-level programming.
A GPU can offer a very high computational rate if the algorithm is well-suited for the device.
One such application is matrix computation.
Matrix applications like the Jacobi relaxation work well in parallel.
As such, it is well suited to work with on GP-GPUâ€™s.

The rest of this report is organized in the following way.
The next section talks about our project idea, design and analysis of solution to our problem.
Section 3 shows the actual implementations followed by the results achieved.
In section 4 we discuss work related to ours and finally conclude in section 5.
We had limited goals due to shortage of time and resources, so we discuss future work in section 6.

    \subsection{Background} %CUDA, math
    The Jacobi Relaxation is a commonly used iterative method for solving systems of equations.
    It uses the main, upper, and lower diagonals of the matrix.
    Able to parallelize the computations on individual elements of the matrix to a large extent.
    This makes it very worthwhile to implement on CUDA architecture.

    The general form for a system of linear equations is (in matrix form):
    \[A*u=f\]
    Our particular implementation of the relaxation uses a weight \(w\) to help it converge more quickly:
    \[x^{(i)} = x^{(i-1)} * (1-w)+w*(D^{-1}(f-(L+U))*x^{(i-1)}\]
    The remaining parameters will now be discussed in more detail.
    \(A\) and \(f\) are the known equation coefficients and constant solutions.
    \(u\) is the unknown solution matrix.
    \(x^{(0)}\) is an initial guess for the matrix u.
    \(x^{(i)}\) is the \(i^{th}\) iteration of the Jacobi relaxation.
    The algorithm is said to be done when the solution converges, or when the solution is within a predetermined error boundary.
    Each previous value of \(x^{(i)}\) is used in the next iteration of the algorithm.
    \(D\) is composed of a matrix of the same size as \(A\), with zeros in all places besides the main diagonal:
    \[
        D =
        \begin{bmatrix*}
        A_{1,1}     & 0         & 0 \\
        0           & \ddots    & 0 \\
        0           & 0         & A_{i,i}
        \end{bmatrix*}
    \]
    \(D^{-1}\) is simply the matrix where each element of \(D\) is inverted:
    \[
        D^{-1} =
        \begin{bmatrix*}
        \frac{1}{A_{1,1}}   & 0         & 0 \\
        0                   & \ddots    & 0 \\
        0                   & 0         & \frac{1}{A_{i,i}}
        \end{bmatrix*}
    \]
    The lower and upper diagonal are the diagonals that run parallel to the main diagonal:
    \[
        A=
        \begin{bmatrix*}
        D & U & x \\
        L & D & U \\
        x & L & D
        \end{bmatrix*}
    \]
    \[
        L=
        \begin{bmatrix*}
        0 & 0               & 0 \\
        A_{0,1} & 0         & 0 \\
        0       & A_{1,2}   & 0
        \end{bmatrix*}
        ,
        U=
        \begin{bmatrix*}
        0 & A_{1,0} & 0       \\
        0 & 0       & A_{2,1} \\
        0 & 0       & 0
        \end{bmatrix*}
    \]
    
    \subsection{System Specifications}
    The device we used to run and benchmark our implementations is an Nvidia Tesla C1060.
    See table ~\ref{tb:tesla} for more detail.

    \begin{table*}[!hb]\centering
        \begin{tabular}{@{}l l@{}}\toprule
        \bf{Spec}                       &   \bf{Value}          \\
        \hline
        Compute Capability              &   1.3                 \\
        Dedicated global memory         &   4294770688 (4GiB)   \\
        Total Constant Memory           &   65536 (64kiB)       \\
        Scalar Processors per GPU       &   240                 \\
        Streaming Multiprocessor Count  &   30                  \\
        Shared memory per MP            &   16384 (16kiB)       \\
        Registers per MP                &   16384 (16kiB)       \\
        Threads per warp                &   32                  \\
        Max threads per block           &   512                 \\
        Max thread dimensions           &   (512 512 64)        \\
        Max Grid dimensions             &   (65535 65535 1)     \\
        \hline
        \end{tabular}
    \caption{Nvidia Tesla C1060 Specifications}
    \label{tb:tesla}
    \end{table*}

\section{Our Project}
    \subsection{Requirements} %hardware, software
    \subsection{Analysis} %how do we do testing, what are we measuring, etc.
    \subsection{Design} %what we did, how did we improve it (performance tuning).  input matrix size, kernel code (including block size), etc.

\section{Results} %prove the idea is good
Only the algorithm and design of to solve this problem have been discussed.
This section describes the actual implementation and results that were obtained using these optimizations.
Two types of optimizations were performed: kernel optimizations, and maximizing the throughput according to the array size.
    \subsection{Implementation 1: One Kernel Optimization}
    %Our first optimization is to implement Jacobi relaxation in CUDA with one kernel.
    The first optimization was to implement a Jacobi relaxation in CUDA with a single kernel.
    %In [reference] Michael has implemented Jacobi relaxation in CUDA using two kernel calls per iteration.
    Michael Wolfe implemented a Jacobi relaxation in CUDA using two kernel calls per iteration \cite{michael}.
    %Instead as an optimization we use a single kernel call per iteration reducing the over head to initiate an extra kernel each time.
    Instead, our design utilized a single kernel call per iteration.
    This reduces the overhead required to initiate an extra kernel each time.
    %The reason for using two kernels initially was to reduce the change values across the blocks to one single value.
    The reason for using two kernels initially was to reduce the change values across the blocks to one single value.
    %These change values were reduced from one change value per thread to a change value per block, in the first kernel 'jacobikernel'.
    These change values were reduced from one change value per thread to a change value per block, in the first kernel 'jacobikernel'.
    The second kernel, which had fewer numbers of threads and one single block, those values are reduced to one single change value.
    %In our implementation we used the existing threads to do the additional work of further reducing the per block change values to a single value.
    In our implementation we used the existing threads to do the additional work of further reducing the per block change values to a single value.
    With this optimization we were able to reduce the calculation time to 10\%.
    %However we could not reduce it much further because though here we are reducing the over head of initialization of a new kernel and creating new threads, but in our implementation there are other threads in the end which sit idle and do not have much work to do.
    Though we can reduce the overhead of initialization a new kernel and creating new threads, there are other threads that remain idle.
    This means that we cannot reduce the overhead much further.
    %However this is a possible way of optimization which did give us positive results.
    However this is a possible way of optimization which did give us positive results.
    In the table below we have shown a few array sizes, their performance with one kernel (our implementation), performance with two kernels \cite{michael} and the performance improvement of one kernel, over two kernel implementation.

   % \begin{table*}[ht]\footnotesize
        %\centering
        %\begin{tabular}{ | p{1.3cm} | p{1.5cm} | p{1.5cm} | p{1.6cm} | p{1.6cm} | p{2cm} | p{1.9cm} | p{2cm} | }
            %\hline
            %Array Size  & One kernel total time (s) & Two kernel total time (s) & One kernel calc. time (s) & Two kernel calc. time (s) & Improvement on total time (s) & Improvement on calc. time (s) & Improvement in calc. time: 1 kernel over 2 kernel (\%) \\ \hline
            %130x130 & 0.108135    & 0.106293    & 0.009158    & 0.010240    & 0.98296574  & 1.11814807  & 10.5  \\
            %146x146 & 0.104811    & 0.106242    & 0.010526    & 0.011699    & 1.01365315  & 1.11143834  & 10    \\
            %162x162 & 0.131627    & 0.115655    & 0.014078    & 0.015089    & 0.87865711  & 1.07181418  & 6.7   \\
            %178x178 & 0.115519    & 0.118052    & 0.018908    & 0.020070    & 1.02192713  & 1.06145547  & 5.7   \\
            %194x194 & 0.119967    & 0.119893    & 0.021031    & 0.022331    & 0.99938316  & 1.06181351  & 5.8   \\
            %210x210 & 0.125773    & 0.128649    & 0.024906    & 0.026340    & 1.02286659  & 1.05757649  & 5.4   \\
            %226x226 & 0.128909    & 0.130154    & 0.029529    & 0.030782    & 1.00965798  & 1.04243286  & 4     \\
            %242x242 & 0.137315    & 0.150276    & 0.034656    & 0.037037    & 1.09438881  & 1.06870383  & 6.4   \\
            %\hline
        %\end{tabular}
        %\caption{Timing results of different Jacobi Relaxation implementations}
        %\label{tab:timing}
    %\end{table*}

    \subsection{Performance Tuning}
    The performance of the GPU is traditionally obtained at the algorithmic level.
    We have measured and simulated the performance by experimenting with various sizes of blocks and matrices.
    Ideally, a larger matrix -size would increase the performance of the GPU because,  it would increase  the  number of computations.
    We have taken the following factors as benchmarks to tune the performance of the Jacobi relaxation in CUDA.
    \begin{itemize}
    \item Time taken
    \item Throughput
    \item GFLOPS
    \item Occupancy
    \item Speed Up
    \end{itemize}
    Time taken by the GPU  : We have calculated the time taken by the GPU in 2 ways
    \begin{enumerate}
    \item Direct measurement
    \item Measurement using a profiler
    \end{enumerate}

    \subsection{Direct Measurement} 
    We have measured the time elapsed through direct measurement using the following 2 ways
    \begin{enumerate}
    \item Gettimeofday: The gettimeofday() function shall obtain the current time, expressed as seconds and microseconds since the Epoch, and store it in the timeval structure pointed to by tv.
    Here is the sample code that we used:

    \begin{minted}[fontsize=\footnotesize]{c}
    gettimeofday(&tt1, NULL);
    JacobiGPU(a, n, m, .2, .1, .1, .1);
    gettimeofday(&tt2, NULL);
    ms = (tt2.tv_sec - tt1.tv_sec);
    ms = ms * 1000000 + (tt2.tv_usec \
        - tt1.tv_usec);
    fms = (float)ms / 1000000.0f;
    printf( "time(gpu ) \
        = %f seconds\n", fms );
    \end{minted}

    \item Cuda Events: The CUDA event API provides calls that create and destroy events, record events (via timestamp), and convert timestamp differences into a floating-point value in milliseconds.
    We have used the cudaEventRecord and the cudaEventElapsedTime to  measure the time taken by the GPU.
    The cudaEventRecord records an event and the cudaEventElapsedTime  computes the elapsed time between two events (in milliseconds with a resolution of around 0.5 microseconds).
    Here is the sample code that we used before and after the kernel call. 

    \begin{minted}[fontsize=\footnotesize]{c}
    cudaEventRecord(e2);
    cudaMemcpy(&change, lchange, \
        sizeof(float), cudaMemcpyDeviceToHost);
    cudaEventElapsedTime(&msec, e1, e2);
    sumtime += msec;
    \end{minted}

    \end{enumerate}

    \subsection{Shared Memory Allocation}%john
    In the most advanced version of Michael Wolfe's code and in the versions that we ended up implementing ourselves, specific shared memory is allocated in each block to accommodate the calculations of each thread.
    Since it is impossible to dynamically allocate memory on the device, we had to allocate enough memory to accommodate the largest number of threads that we would be using.
    Since the threads were allocated in a 2 dimensional square and the maximum number of threads was 512, we had to allocate enough memory for a 22 by 22 square block.
    This memory was allocated into the shared memory of each block irrespective of how much of that memory was actually used.
    It was only necessary to make this change for program versions that allowed for different numbers of threads per block of course.

    As you can see from the occupancy graph this had the effect of increasing the occupancy of the programs allowing for multiple threads.
    However these programs achieved consistently lower GFLOPS as well as higher GPU calculation times.
    This illustrates the fact that an increase in occupancy does not automatically lead to an increase in performance.
    In our case it is affirming out that the bottleneck in our program is not in memory.
    At any given time there is probably a thread that has already fetched its required memory and is ready to run on the SM.

    \subsection{Comparing Block Sizes}
    One of the most basic optimizations that can be done is to vary the block size your code runs on.
    In most cases as the number of threads running on a block increase so does the performance.
    This is mostly because while GPUs are exceptional at performing many small calculations very quickly, they do not excel at memory access.
    This is a general rule however.
    The goal is to maximize the fraction of SM resources that are being used at any given time.
    One guideline is that as the complexity of the kernel increases the thread count should decrease, but this is not always beneficial.

    As can be seen from the data collected performance is generally increased across all benchmarks as the thread count goes up.
    This increase has a limit, however.
    Once the threads begin to exceed 16x16 we begin to see a decrease in performance, as measured by GFLOPs and time taken to by the GPU to complete all its calculations.
    This is because to achieve maximum performance our programs use tiling, which almost always performs best when 256 (16 x 16) threads are used.
    This is because in devices of compute capability 1.x global memory is accessed in chunks of half a warp or 16 threads.
    By using a number of threads that is a multiple of 16 you can maximize the amount of memory read per memory fetch.
    Since a block requires a square matrix in order to perform the jacobi relaxation algorithm, a 16x16 sized block will minimize the number of clock cycles required to access global memory.

    %\begin{table}\footnotesize
    %\centering
        %\begin{tabular}{ | c | c | c | }
        %\hline
        %Matrix Size & Block Size & Time Taken \\
        %\hline
        %1 & 2 & 3 \\
        %\hline
        %\end{tabular}
        %\caption{Inputs and time taken by the GPU}
        %\label{tab:inputs_time_taken}
    %\end{table}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/a1.pdf}
        \caption{}
        \label{fig:time_taken1}
    \end{figure*}
    
    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/b1.pdf}
        \caption{}
        \label{fig:time_taken2}
    \end{figure*}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/c1.pdf}
        \caption{}
        \label{fig:time_taken3}
    \end{figure*}

    \subsection{Throughput}
    Throughput, here is typically the amount of work the GPU does in a given amount of time.
    We have used the computeprof visual profiler to measure the memory and instructional throughput.
    The memory throughput reported in the summary table of computeprof ,  measures  the throughput using a subset of the GPUâ€™s multiprocessors and then extrapolates that number to the entire GPU, thus reporting an estimate of the data throughput.
    Also, the throughput reported by the profiler includes the transfer of data not used by the kernel.
    Here is the table of input and the throughput gained.

  %  \begin{table}\footnotesize
    %\centering
        %\begin{tabular}{ | c | c | p{1.2cm} | p{1.5cm} | p{1.0cm} | }
        %\hline
        %Matrix Size & Block Size & Kernel GPU Time & Global Mem Overall Throughput & Instruction Throughput \\
        %\hline
        %\end{tabular}
        %\caption{Input and throughput gained}
        %\label{tab:input_throughput}
    %\end{table}

    \subsection{GFLOPS}
    FLOPS are typically the measure of a computerâ€™s performance.
    This is similar to the number of instructions per second, but we consider only the floating point operations.
    We have benchmarked the GFLOPS based on the maximum performance that our CUDA device could perform.
    The Tesla C 1060 can perform ?????.
    We estimated the effective FLOPS at the algorithmic level and multiplied it by the number of iterations, Block Size, Grid Size and arrived at the total number of FLOPS and hence calculated the GFLOPS.
    We then compared the gained GFLOPS with the benchmark.
    Here are the tabulated results.

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/a2.pdf}
        \caption{}
        \label{fig:gflops1}
    \end{figure*}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/b2.pdf}
        \caption{}
        \label{fig:gflops2}
    \end{figure*}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/c2.pdf}
        \caption{}
        \label{fig:gflops3}
    \end{figure*}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/g1.pdf}
        \caption{}
        \label{fig:jacobi_instr_throughput}
    \end{figure*}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/g2.pdf}
        \caption{}
        \label{fig:reduction_instr_throughput}
    \end{figure*}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/g3.pdf}
        \caption{}
        \label{fig:jacobi_mem_throughput}
    \end{figure*}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/g4.pdf}
        \caption{}
        \label{fig:reduction_mem_throughput}
    \end{figure*}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/o1.pdf}
        \caption{}
        \label{fig:1k_occupancy}
    \end{figure*}

    \begin{figure*}
        \centering
        \includegraphics[width=8cm]{graphs/o2.pdf}
        \caption{}
        \label{fig:2k_occupancy}
    \end{figure*}

    \subsection{Occupancy}
    Executing other warps when one warp is paused is the only way to hide latencies and keep the hardware busy.
    Occupancy is related to the number of active warps on a multiprocessor and is therefore important in determining how effectively the hardware is kept busy.
    We used the CUDA occupancy calculator and the Compute Profiler  to calculate the multiprocessor occupancy of a GPU by a given CUDA kernel.
    Maximizing the occupancy can help to cover latency during global memory loads that are followed by a \_\_syncthreads().
    The occupancy is determined by the amount of shared memory and registers used by each thread block.

\subsection{Summary} % of results with graphs/tables/etc.

\section{Related Work} %prove the idea is new? It really isn't, but it's our take on existing software.  This section may be small.
    \subsection{michael wolfe!!}
    \subsection{mention} %other papers we read in class?

\section{Conclusion} %repeat idea, summarize key results
We have described our techniques for optimizing the code we received from the presentation given by Michael Wolfe.
We have also shown our methods for collecting data about the performance of the code that was received and our own extension of that code.
We have found that to optimize code that is not hardware specific is very difficult.
In some areas our implementation does perform better but in others it does not.
The results that were obtained does suggest that it would be possible to increase the speed of the calculations farther, although we were not able to implement them further at this time.

\section{Future Work} %list the things you wanted to do but couldn't finish in time for this paper
There are several areas for future work that are recognized at this time.
Additional work on implementing the Jacobi Method with one kernel call.
We believe that better performance can still be achieved using the current method with further effort put into optimization.
Another area of work would be to implement a way for the user to manually enter the number of threads to be used.
This would allow for the rapid testing of different number of threads to view the most efficient.
We would have also like to have added functionality to initialize the array on the GPU.
Thus, allowing it to be done in parallel, and therefore, done more quickly than the sequential version as it is now.
Lastly, creating a version that would customize itself to each individual GPU by getting information from the GPU about itself and using this data to adjust the function to better fit the GPU.

\section{Acknowledgements} %mention wolfe et al
We would like to thank Michael Wolfe for the examples that he provided for us.
This work was performed on equipment allocated to us from the CUDA lab.

\end{multicols}

\begin{flushleft}
\begin{thebibliography}{99}
\topmargin = -100pt
    \bibitem{wiki}``Relaxation (iterative Method).''
        Wikipedia, the Free Encyclopedia. Web. 01 Aug. 2011. $<$http://en.wikipedia.org/wiki/Relaxation\_(iterative\_method)$>$.
    \bibitem{michael}``Jacobi Relaxation''
        Michael Wolfe. The Portland Group, Inc. $<$http://www.pgroup.com/$>$
\end{thebibliography}
\end{flushleft}

\clearpage


\section{Appendix A: Code}

\subsection{Sequential Code}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/seq_jacobi.c}

\subsection{Michael Wolfe's Code}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/original_jacobi5.cu}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/original_jacobi6.cu}

\subsection{One Kernel Implementation}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/1k_jacobi5.cu}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/1k_jacobi6.cu}

\subsection{Adjustable Thread Count Implementation}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/orig_J6M_v2.cu}
\inputminted[linenos, fontsize=\footnotesize]{c}{../jacobi_final/1k_J6M_v2.cu}

\end{document}
